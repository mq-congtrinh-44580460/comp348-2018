{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Classification of Business Reviews\n",
    "\n",
    "**Submission deadline: Friday 27 April 2018, 11pm**\n",
    "\n",
    "**Penalty for late submission: 4 marks per day**\n",
    "\n",
    "**Assessment marks: 20 marks (20% of the total unit assessment)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code reads business reviews which are part of the [Yelp Dataset stored in Kaggle](https://www.kaggle.com/yelp-dataset/yelp-dataset). The data are stored in a CSV file. The following code reads the CSV file and prints the contents of the first 5 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>date</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vkVSCC7xljjrAI4UGfnKEQ</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>AEx2SYEUJmTxVVB18LlCwA</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Super simple place but amazing nonetheless. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n6QzIUObkYshz4dz2QRJTw</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>VR6GpWIda3SfvPC-lg9H3w</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Small unassuming place that changes their menu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MV3CcKScW05u5LVfF6ok0g</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>CKC0-MOWMqoeWf6s-szl8g</td>\n",
       "      <td>5</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Lester's is located in a beautiful neighborhoo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IXvOzsEMYtiJI0CARmj77Q</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>ACFtxLv8pGrrxMm6EgjreA</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Love coming here. Yes the place always needs t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L_9BTb55X0GDtThi6GlZ6w</td>\n",
       "      <td>bv2nCi5Qv5vroFiqKGopiw</td>\n",
       "      <td>s2I_Ni76bjJNK9yG60iD-Q</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>Had their chocolate almond croissant and it wa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                review_id                 user_id             business_id  \\\n",
       "0  vkVSCC7xljjrAI4UGfnKEQ  bv2nCi5Qv5vroFiqKGopiw  AEx2SYEUJmTxVVB18LlCwA   \n",
       "1  n6QzIUObkYshz4dz2QRJTw  bv2nCi5Qv5vroFiqKGopiw  VR6GpWIda3SfvPC-lg9H3w   \n",
       "2  MV3CcKScW05u5LVfF6ok0g  bv2nCi5Qv5vroFiqKGopiw  CKC0-MOWMqoeWf6s-szl8g   \n",
       "3  IXvOzsEMYtiJI0CARmj77Q  bv2nCi5Qv5vroFiqKGopiw  ACFtxLv8pGrrxMm6EgjreA   \n",
       "4  L_9BTb55X0GDtThi6GlZ6w  bv2nCi5Qv5vroFiqKGopiw  s2I_Ni76bjJNK9yG60iD-Q   \n",
       "\n",
       "   stars        date                                               text  \\\n",
       "0      5  2016-05-28  Super simple place but amazing nonetheless. It...   \n",
       "1      5  2016-05-28  Small unassuming place that changes their menu...   \n",
       "2      5  2016-05-28  Lester's is located in a beautiful neighborhoo...   \n",
       "3      4  2016-05-28  Love coming here. Yes the place always needs t...   \n",
       "4      4  2016-05-28  Had their chocolate almond croissant and it wa...   \n",
       "\n",
       "   useful  funny  cool  \n",
       "0       0      0     0  \n",
       "1       0      0     0  \n",
       "2       0      0     0  \n",
       "3       0      0     0  \n",
       "4       0      0     0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd_data = pd.read_csv('yelp_review.zip')\n",
    "pd_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data, we will only use the reviews and the star rating. The following code extracts this information and places it in a list of pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_data = list(zip(pd_data['text'], pd_data['stars']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5261668"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"Super simple place but amazing nonetheless. It's been around since the 30's and they still serve the same thing they started with: a bologna and salami sandwich with mustard. \\n\\nStaff was very helpful and friendly.\",\n",
       "  5),\n",
       " (\"Small unassuming place that changes their menu every so often. Cool decor and vibe inside their 30 seat restaurant. Call for a reservation. \\n\\nWe had their beef tartar and pork belly to start and a salmon dish and lamb meal for mains. Everything was incredible! I could go on at length about how all the listed ingredients really make their dishes amazing but honestly you just need to go. \\n\\nA bit outside of downtown montreal but take the metro out and it's less than a 10 minute walk from the station.\",\n",
       "  5),\n",
       " (\"Lester's is located in a beautiful neighborhood and has been there since 1951. They are known for smoked meat which most deli's have but their brisket sandwich is what I come to montreal for. They've got about 12 seats outside to go along with the inside. \\n\\nThe smoked meat is up there in quality and taste with Schwartz's and you'll find less tourists at Lester's as well.\",\n",
       "  5),\n",
       " (\"Love coming here. Yes the place always needs the floor swept but when you give out  peanuts in the shell how won't it always be a bit dirty. \\n\\nThe food speaks for itself, so good. Burgers are made to order and the meat is put on the grill when you order your sandwich. Getting the small burger just means 1 patty, the regular is a 2 patty burger which is twice the deliciousness. \\n\\nGetting the Cajun fries adds a bit of spice to them and whatever size you order they always throw more fries (a lot more fries) into the bag.\",\n",
       "  4),\n",
       " (\"Had their chocolate almond croissant and it was amazing! So light and buttery and oh my how chocolaty.\\n\\nIf you're looking for a light breakfast then head out here. Perfect spot for a coffee\\\\/latt√© before heading out to the old port\",\n",
       "  4)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the distribution of star ratings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({5: 2253347, 4: 1223316, 3: 615481, 1: 731363, 2: 438161})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter([rating for text, rating in all_data])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 5 artists>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAD8CAYAAACyyUlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAADxJJREFUeJzt3W+onvV9x/H3Z6bdxNaZzrMQTLpTtlBwwqw9aKBldJXFqGVxIGJhNRTXDFRo2WBL9yRbuwfZg7VD6AQ3g8nW1UlbMVRtGqxQCkvrSWv9W/HQRUywJjVWWwortt89OL9st9k5OX9+yblyPO8X3NzX/b1+1/X7Xo8+5/pz3ydVhSRJPX5l6AYkScufYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqduqoRtYKhdeeGGNj48P3YYkLSsHDx78UVWNzTVuxYTJ+Pg4k5OTQ7chSctKkufnM87LXJKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuK+Yb8JK0EOPbHxi6hdPm0M5rz/gcnplIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG5zhkmS9UkeSfJ0kqeSfLzV35Fkf5Ln2vvqVk+S25NMJXk8yWUj+9raxj+XZOtI/b1Jnmjb3J4ki51DkrT05nNm8jrwF1V1MbARuDXJxcB24OGq2gA83D4DXA1saK9twB0wHQzADuAK4HJgx4lwaGM+NrLd5lZf0BySpGHMGSZV9WJVfact/wR4BrgI2ALsbsN2A9e15S3Anpp2ALggyVrgKmB/VR2vqleA/cDmtu78qjpQVQXsOWlfC5lDkjSABd0zSTIOvAf4FrCmql5sq34IrGnLFwEvjGx2uNVOVT88Q51FzCFJGsC8wyTJ24AvAZ+oqtdG17UzijrNvb3BYuZIsi3JZJLJY8eOnaHOJEnzCpMkb2E6SD5fVV9u5ZdOXFpq70db/QiwfmTzda12qvq6GeqLmeMNqurOqpqoqomxsbH5HKokaRHm8zRXgLuAZ6rqMyOr9gInnsjaCtw/Ur+pPXG1EXi1XaraB2xKsrrdeN8E7GvrXkuysc1100n7WsgckqQBrJrHmPcBHwGeSPJYq/01sBO4N8nNwPPADW3dg8A1wBTwM+CjAFV1PMmngUfbuE9V1fG2fAtwN3Au8FB7sdA5JEnDmDNMquqbQGZZfeUM4wu4dZZ97QJ2zVCfBC6Zof7yQueQJC09vwEvSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSeo2Z5gk2ZXkaJInR2p/k+RIksfa65qRdZ9MMpXk2SRXjdQ3t9pUku0j9Xcl+Var/0eSt7b6r7bPU239+FxzSJKGMZ8zk7uBzTPUP1tVl7bXgwBJLgZuBH63bfNPSc5Jcg7wOeBq4GLgw20swN+3ff0O8Apwc6vfDLzS6p9t42adY2GHLUk6neYMk6r6BnB8nvvbAtxTVf9dVf8FTAGXt9dUVf2gqn4O3ANsSRLgg8AX2/a7getG9rW7LX8RuLKNn20OSdJAeu6Z3Jbk8XYZbHWrXQS8MDLmcKvNVv8N4MdV9fpJ9Tfsq61/tY2fbV+SpIEsNkzuAH4buBR4EfiH09bRaZRkW5LJJJPHjh0buh1JetNaVJhU1UtV9Yuq+iXwz/zfZaYjwPqRoetabbb6y8AFSVadVH/Dvtr6X2/jZ9vXTH3eWVUTVTUxNja2mEOVJM3DosIkydqRj38MnHjSay9wY3sS613ABuDbwKPAhvbk1luZvoG+t6oKeAS4vm2/Fbh/ZF9b2/L1wNfb+NnmkCQNZNVcA5J8AfgAcGGSw8AO4ANJLgUKOAT8GUBVPZXkXuBp4HXg1qr6RdvPbcA+4BxgV1U91ab4K+CeJH8HfBe4q9XvAv41yRTTDwDcONcckqRhZPqP/Te/iYmJmpycHLoNScvE+PYHhm7htDm089pFb5vkYFVNzDXOb8BLkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSeq2augGJJ29xrc/MHQLp8WhndcO3cKbnmcmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkrrNGSZJdiU5muTJkdo7kuxP8lx7X93qSXJ7kqkkjye5bGSbrW38c0m2jtTfm+SJts3tSbLYOSRJw5jPmcndwOaTatuBh6tqA/Bw+wxwNbChvbYBd8B0MAA7gCuAy4EdJ8KhjfnYyHabFzOHJGk4c4ZJVX0DOH5SeQuwuy3vBq4bqe+paQeAC5KsBa4C9lfV8ap6BdgPbG7rzq+qA1VVwJ6T9rWQOSRJA1nsPZM1VfViW/4hsKYtXwS8MDLucKudqn54hvpi5pAkDaT7Bnw7o6jT0MtpnyPJtiSTSSaPHTt2BjqTJMHiw+SlE5eW2vvRVj8CrB8Zt67VTlVfN0N9MXP8P1V1Z1VNVNXE2NjYgg5QkjR/iw2TvcCJJ7K2AveP1G9qT1xtBF5tl6r2AZuSrG433jcB+9q615JsbE9x3XTSvhYyhyRpIKvmGpDkC8AHgAuTHGb6qaydwL1JbgaeB25owx8ErgGmgJ8BHwWoquNJPg082sZ9qqpO3NS/heknxs4FHmovFjqHJGk4c4ZJVX14llVXzjC2gFtn2c8uYNcM9UngkhnqLy90DknSMPwGvCSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkbquGbmA5GN/+wNAtnDaHdl47dAuS3oQ8M5EkdTNMJEndDBNJUjfDRJLUzTCRJHUzTCRJ3QwTSVI3w0SS1M0wkSR18xvw0in46wfS/HhmIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6dYVJkkNJnkjyWJLJVntHkv1Jnmvvq1s9SW5PMpXk8SSXjexnaxv/XJKtI/X3tv1PtW1zqjkkScM4HWcmf1BVl1bVRPu8HXi4qjYAD7fPAFcDG9prG3AHTAcDsAO4Argc2DESDncAHxvZbvMcc0iSBnAmLnNtAXa35d3AdSP1PTXtAHBBkrXAVcD+qjpeVa8A+4HNbd35VXWgqgrYc9K+ZppDkjSA3jAp4GtJDibZ1mprqurFtvxDYE1bvgh4YWTbw612qvrhGeqnmkOSNIDen1N5f1UdSfKbwP4k3x9dWVWVpDrnOKVTzdECbhvAO9/5zjPZhiStaF1nJlV1pL0fBe5j+p7HS+0SFe39aBt+BFg/svm6VjtVfd0MdU4xx8n93VlVE1U1MTY2ttjDlCTNYdFhkuS8JG8/sQxsAp4E9gInnsjaCtzflvcCN7WnujYCr7ZLVfuATUlWtxvvm4B9bd1rSTa2p7huOmlfM80hSRpAz2WuNcB97WndVcC/V9VXkzwK3JvkZuB54IY2/kHgGmAK+BnwUYCqOp7k08Cjbdynqup4W74FuBs4F3iovQB2zjKHJGkAiw6TqvoB8Hsz1F8GrpyhXsCts+xrF7BrhvokcMl855AkDcNvwEuSuvnPsTQn/0GUpLl4ZiJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkboaJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRuhokkqZthIknqZphIkroZJpKkbss6TJJsTvJskqkk24fuR5JWqmUbJknOAT4HXA1cDHw4ycXDdiVJK9OyDRPgcmCqqn5QVT8H7gG2DNyTJK1IyzlMLgJeGPl8uNUkSUssVTV0D4uS5Hpgc1X9afv8EeCKqrptZMw2YFv7+G7g2SVvdGEuBH40dBMDWcnHDiv7+D32s9tvVdXYXINWLUUnZ8gRYP3I53Wt9r+q6k7gzqVsqkeSyaqaGLqPIazkY4eVffwe+5vj2JfzZa5HgQ1J3pXkrcCNwN6Be5KkFWnZnplU1etJbgP2AecAu6rqqYHbkqQVadmGCUBVPQg8OHQfp9GyuSR3BqzkY4eVffwe+5vAsr0BL0k6eyzneyaSpLOEYXIWSLIrydEkTw7dy1JLsj7JI0meTvJUko8P3dNSSfJrSb6d5Hvt2P926J6WWpJzknw3yVeG7mWpJTmU5IkkjyWZHLqfXl7mOgsk+X3gp8Ceqrpk6H6WUpK1wNqq+k6StwMHgeuq6umBWzvjkgQ4r6p+muQtwDeBj1fVgYFbWzJJ/hyYAM6vqg8N3c9SSnIImKiqs/17JvPimclZoKq+ARwfuo8hVNWLVfWdtvwT4BlWyC8Z1LSfto9vaa8V89ddknXAtcC/DN2L+hkmOmskGQfeA3xr2E6WTrvM8xhwFNhfVSvm2IF/BP4S+OXQjQykgK8lOdh+rWNZM0x0VkjyNuBLwCeq6rWh+1kqVfWLqrqU6V9wuDzJirjMmeRDwNGqOjh0LwN6f1VdxvQvn9/aLncvW4aJBtfuF3wJ+HxVfXnofoZQVT8GHgE2D93LEnkf8EftvsE9wAeT/NuwLS2tqjrS3o8C9zH9S+jLlmGiQbWb0HcBz1TVZ4buZyklGUtyQVs+F/hD4PvDdrU0quqTVbWuqsaZ/imkr1fVnwzc1pJJcl574IQk5wGbgGX9NKdhchZI8gXgP4F3Jzmc5Oahe1pC7wM+wvRfpo+11zVDN7VE1gKPJHmc6d+a219VK+4R2RVqDfDNJN8Dvg08UFVfHbinLj4aLEnq5pmJJKmbYSJJ6maYSJK6GSaSpG6GiSSpm2EiSepmmEiSuhkmkqRu/wOGOpCoyjh8fQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(range(1,6), [c[1], c[2], c[3], c[4], c[5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In this assignment you will predict whether a particular review gives 5 stars or not.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set is fairly large with more than 5 million samples. To speed up the computations for this assigmnent, we will use 500,000 samples for training,  10,000 for the dev-test set and 10,000 for the test set. To reduce any possible bias while partitioning the data set, we will first shuffle the data and then partition into training data, dev-test data, and test data using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(1234)\n",
    "random.shuffle(all_data)\n",
    "train_data, devtest_data, test_data = all_data[:500000], all_data[500000:510000], all_data[510000:520000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 (1 mark)\n",
    "The data are annotated with a star rating. In this assignment we will attempt to predict whether the review has 5 stars or not. In other words, we will use two categories: \"it does not have 5 stars\", and \"it has 5 stars\". According to these categories, check that the training data, devtest data and test data have the same proportions of the categories \"it does not have 5 stars\", and \"it has 5 stars\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set:  Counter({'it does not have 5 stars': 285795, 'it has 5 stars': 214205})\n",
      "Devtest Set:  Counter({'it does not have 5 stars': 5725, 'it has 5 stars': 4275})\n",
      "Test Set:  Counter({'it does not have 5 stars': 5676, 'it has 5 stars': 4324})\n"
     ]
    }
   ],
   "source": [
    "def rate_feature(rate):\n",
    "    if rate == 5:\n",
    "        return 'it has 5 stars'\n",
    "    else:\n",
    "        return 'it does not have 5 stars'\n",
    "\n",
    "from collections import Counter\n",
    "train_count = Counter([rate_feature(rate) for review, rate in train_data])\n",
    "devtest_count = Counter([rate_feature(rate) for review, rate in devtest_data])\n",
    "test_count = Counter([rate_feature(rate) for review, rate in test_data])\n",
    "\n",
    "print(\"Train Set: \", train_count)\n",
    "print(\"Devtest Set: \",devtest_count)\n",
    "print(\"Test Set: \", test_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 (2 marks)\n",
    "Use sklearn to generate the tf.idf matrix of the training set. With this matrix, train an sklearn Naive Bayes classifier using the training set and report the F1 scores of the training set, the devtest set, and the set set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score:  [0.82728917 0.73072612]\n",
      "Devtest F1 score:  [0.8271514  0.72890715]\n",
      "Test F1 score:  [0.82814949 0.73562635]\n"
     ]
    }
   ],
   "source": [
    "text_train = [(review, rate_feature(rate)) for review, rate in train_data]\n",
    "text_devtest = [(review, rate_feature(rate)) for review, rate in devtest_data]\n",
    "text_test = [(review, rate_feature(rate)) for review, rate in test_data]\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(input='contents', stop_words='english', max_features=2000)\n",
    "text_tfidf_train = tfidf.fit_transform([x for x, y in text_train])\n",
    "text_tfidf_devtest = tfidf.transform([x for x, y in text_devtest])\n",
    "text_tfidf_test = tfidf.transform([x for x, y in text_test])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "sklearn_tfidfclassifier = MultinomialNB()\n",
    "sklearn_tfidfclassifier.fit(text_tfidf_train, [y for x, y in text_train])\n",
    "\n",
    "tfidf_Train_pred = sklearn_tfidfclassifier.predict(text_tfidf_train)\n",
    "tfidf_Dev_pred = sklearn_tfidfclassifier.predict(text_tfidf_devtest)\n",
    "tfidf_Test_pred = sklearn_tfidfclassifier.predict(text_tfidf_test)\n",
    "\n",
    "print('Train F1 score: ', f1_score(tfidf_Train_pred, [y for x, y in text_train], average=None))\n",
    "print('Devtest F1 score: ', f1_score(tfidf_Dev_pred, [y for x, y in text_devtest], average=None))\n",
    "print('Test F1 score: ', f1_score(tfidf_Test_pred, [y for x, y in text_test], average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Exercise 3 (2 marks)\n",
    "Logistic regression normally produces better results than Naive Bayes but it takes longer time to train. Look at the documentation of sklearn and train a logistic regression classifier using the same tfidf information as in exercise 2. Report the F1 scores of the training set, the devtest set, and the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train F1 score:  [0.8425473  0.78459826]\n",
      "Devtest F1 score:  [0.83820225 0.77793594]\n",
      "Test F1 score:  [0.8402223  0.78312117]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(text_tfidf_train, [y for x, y in text_train])\n",
    "\n",
    "logreg_Train_pred = logreg.predict(text_tfidf_train)\n",
    "logreg_Dev_pred = logreg.predict(text_tfidf_devtest)\n",
    "logreg_Test_pred = logreg.predict(text_tfidf_test)\n",
    "\n",
    "print('Train F1 score: ', f1_score(logreg_Train_pred, [y for x, y in text_train], average=None))\n",
    "print('Devtest F1 score: ', f1_score(logreg_Dev_pred, [y for x, y in text_devtest], average=None))\n",
    "print('Test F1 score: ', f1_score(logreg_Test_pred, [y for x, y in text_test], average=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4 (4 marks)\n",
    "Given the results obtained in the previous exercises, answer the following questions. You must justify all answers.\n",
    "1. (1 mark) How much overfitting did you observe in the classifiers?\n",
    "2. (1 mark) What would you do to reduce overfitting?\n",
    "3. (1 mark) Which classifier is better?\n",
    "4. (1 mark) What can you conclude from the differences in the results between the dev-test set and the test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(write your answer here udsing [Markdown formatting](http://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html))\n",
    "\n",
    "    1.\n",
    "        There are significant numbers of overfitting especially in Logistic Regression. There are noise in data that cause the overfitting. \n",
    "    2. \n",
    "        Reduce the noise in data so that the train set can create a better fitting diagram.\n",
    "    3. \n",
    "        Logistic regression is better because it provides better chance to predict due to the computed scores.\n",
    "    4.\n",
    "        Due to the smaller size of test set, dev-set is harder to predict. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5 (2 marks)\n",
    "Write code that counts the false positives and false negatives of the training set of each classifier. What can you conclude from such counts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Neg:  252008  False Pos:  71435  False Neg:  33787  True Pos:  142770\n",
      "True Neg:  243374  False Pos:  48541  False Neg:  42421  True Pos:  165664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "tn_train_tfidf, fp_train_tfidf, fn_train_tfidf, tp_train_tfidf = confusion_matrix(tfidf_Train_pred, [y for x, y in text_train]).ravel()\n",
    "\n",
    "tn_train_logreg, fp_train_logreg, fn_train_logreg, tp_train_logreg = confusion_matrix(logreg_Train_pred, [y for x, y in text_train]).ravel()\n",
    "\n",
    "print(\"True Neg: \",tn_train_tfidf,' False Pos: ', fp_train_tfidf,' False Neg: ', fn_train_tfidf, ' True Pos: ', tp_train_tfidf)\n",
    "print(\"True Neg: \",tn_train_logreg,' False Pos: ', fp_train_logreg,' False Neg: ', fn_train_logreg, ' True Pos: ', tp_train_logreg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (9 marks) - Improve the System and Final Analysis\n",
    "This exercise is open ended. Your goal is to perform a more detailed error analysis and identify ways to improve the classification of reviews **by adding or changing the features**. To obtain top marks in this part, your answer must address all of the following topics:\n",
    "\n",
    "1. An error analysis of the previous systems.\n",
    "2. Based on the error analysis, explain what sort of modifications you would want to implement, and justify why these would be useful modifications.\n",
    "3. Implementation of the improved classifier.\n",
    "4. Evaluation of the results and comparison with the previous classifiers. What system is best and why?\n",
    "5. Explain what further changes would possibly improve the classifier and why.\n",
    "\n",
    "All this information should be inserted in this notebook below this question. The information should be structured in sections and it must be clear and precise. The explanations should be convincing. Below is a possible list of section headings. These sections are just a guideline. Feel free to change them, but make sure that they are informative and relevant.\n",
    "\n",
    "** Note that, even if the new system might not obtain better results than the previous systems, you can obtain top marks if you perform a good error analysis of the initial systems and the final system and you give a sensible justification of the decisions that led you to implement the new system. Similarly, you may not obtain top marks if you present a system that improves on the results but you do not provide a good error analysis or you do not justify your choice of new system. **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Error Analysis\n",
    "\n",
    "Noise in data: there are a certain number of noise in data that effect the train set so that the classifier could not improve it's predictions.\n",
    "\n",
    "### 2. Explanation of the Proposed New Classifier\n",
    "\n",
    "By using nltk.pos_tag, the feature can filter out certain amount of words such as adposition, conjunction, determiner etc. With smaller and unique pools of words, the program is expected to classify more accurately. In theory, a unique pool of words can provide a better chance at predicting.  \n",
    "\n",
    "### 3. Code of the Proposed New Classifier\n",
    "~~~~\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "import collections\n",
    "regex =re.compile('(RB.|VB.|JJ.|NN|NNS)$')\n",
    "\n",
    "\n",
    "def process_review(review):\n",
    "\tstopset = stopwords.words(\"english\")\n",
    "\ttokens = nltk.word_tokenize(review)\n",
    "\ttagged = nltk.pos_tag(tokens)\n",
    "\tresult = []\n",
    "\tfor word, tag in tagged: \n",
    "\t\tif regex.match(tag) and word not in stopset:\n",
    "\t\t\tresult.append(word.lower())\n",
    "\treturn result\n",
    "\n",
    "\n",
    "def proposed_process(start, end):\n",
    "\tresult = []\n",
    "\tfor review, rate in train_data[start:end]:\n",
    "\t\ttemp = (process_review(review), rate_feature(rate))\n",
    "\t\tresult.append(temp)\n",
    "\treturn result\n",
    "\n",
    "\n",
    "##proposed_train = [(process_review(review), rate_feature(rate)) for review, rate in train_data[:250000]]\n",
    "\n",
    "p = Pool(8)\n",
    "start = time.time()\n",
    "proposed=[]\n",
    "word=[]\n",
    "proposed = (p.starmap(proposed_process, [(0, 100000),(100000, 200000)))\n",
    "\n",
    "##Use 200000 train set\n",
    "\n",
    "proposed_train = proposed[0] + proposed[1] + proposed[2]\n",
    "end = time.time()\n",
    "print(end-start)\n",
    "proposed_devtest = [(process_review(review), rate_feature(rate)) for review, rate in devtest_data]\n",
    "proposed_test = [(process_review(review), rate_feature(rate)) for review, rate in test_data]\n",
    "\n",
    "counter = collections.Counter([w for review, rate in proposed_train\n",
    "\t\t\t\t\tfor w in review])\n",
    "\n",
    "top2000words = [w for (w,count) in counter.most_common(2000)]\n",
    "\n",
    "def document_features(words):\n",
    "\t\"Return the document features for an NLTK classifier\"\n",
    "\twords_lower = [w.lower() for w in words]\n",
    "\tresult = dict()\n",
    "\tfor w in top2000words:\n",
    "\t\tresult['has(%s)' % w] = (w in words_lower)\n",
    "\treturn result\n",
    "\n",
    "train_features = [(document_features(x), y) for x, y in train_data]\n",
    "devtest_features = [(document_features(x), y) for x, y in proposed_devtest]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_features)\n",
    "~~~~\n",
    "### 4. Evaluation and Comparison\n",
    "\n",
    "The data is too big for the proposed classifier to run effienciently. It requires a powerful CPU to be executed.\n",
    "With smaller train set (100000 and 200000), tfidf and Logistic Regression still shows better chance at predicting, around 80% comparing with 65% with proposed classifier.  \n",
    "\n",
    "### 5. Final Conclusions and Possible Improvements\n",
    "\n",
    "The proposed classifier is supposed to work more efficently in theory (resistant to overfitting.) With this approach, it sacrifices CPU power and time to obtain better pool of words. \n",
    "\n",
    "With better and faster word proccesors to cut down a significant unnecessary words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission of Results\n",
    "\n",
    "Your submission should consist of this Jupyter notebook with all your code and explanations inserted in the notebook. The notebook should contain the output of the runs so that it can be read by the assessor without needing to run the code.\n",
    "\n",
    "Examine this notebook so that you can have an idea of how to format text for good visual impact. You can also read this useful [guide to the markdown notation](http://daringfireball.net/projects/markdown/syntax), which explains the format of the text.\n",
    "\n",
    "Late submissions will have a penalty of **4 marks deduction per day late**.\n",
    "\n",
    "Each question specifies a mark. The final mark of the assignment is the sum of all the individual marks, after applying any deductions for late submission.\n",
    "\n",
    "By submitting this assignment you are acknowledging that this is your own work. Any submissions that break the code of academic honesty will be penalised as per the [academic honesty policy](https://staff.mq.edu.au/work/strategy-planning-and-governance/university-policies-and-procedures/policies/academic-honesty)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
